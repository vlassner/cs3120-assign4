{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign 4: Model Selection\n",
    "\n",
    "In this lab you will not only train several models, but you'll look at the process needed to answer one of the most important (and difficult) questions, which is:\n",
    "* \"_**How good will this model be at making future predictions**_?\"\n",
    "\n",
    "To answer this question we'll look back at some performance metrics we have seen, and also talk about model underfitting/overfitting and the bias-variance tradeoff. These latter two concepts are general and can be difficult to articulate, but are really important in the world of ML. They also often show up in job interviews for Data Science and/or Machine Learning, so it's important that you understand them well enough to be able to describe/explain what they are to someone else. Although this is succintly referred to as \"_Model Selection_\", we can also think of this as trying better understand a model's future prediction accuracy. \n",
    "\n",
    "*Note that this notebook uses the [Ames, Iowa housing dataset](https://jse.amstat.org/v19n3/decock.pdf), which was featured in the [Kaggle](https://www.kaggle.com/) [Housing Prices Competition](https://www.kaggle.com/competitions/home-data-for-ml-course).*\n",
    "\n",
    "\n",
    "To start, we'll load the Python modules to be used in the lab. If you recall in the last lab, we loaded each module just before it was used, but tha was only for us to see which modules were used for what. This approach of loading them all at the beginning is the typical practice of anyone creating a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "It cannot be stated enough, but getting acquainted with your data before beginning any analysis or modeling task is critical. If you don't, you could easily spend a lot of time attempting to fit a certain type of model to your data, only to later find out that there is something about the data that would have made you choose a different approach. \n",
    "\n",
    "Let's now quickly open the file and use `df.describe()` to see what it looks like. The response or target variable that we will be modeling is `SalePrice`, which is the very last column in the data (so scroll all the way to the right to see what this variable looks like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/sgeinitz/CS3120/refs/heads/main/lab2_data_ames_housing.csv')\n",
    "print(f\"** df has {df.shape[0]} rows and {df.shape[1]} columns **\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other important first step is to see a few rows of the raw data itself using `df.head()`. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With so many columns it can be hard to get an idea of which features there are, let alone understand how they are distributed. One other pandas method that can be useful to better understand how discrete features are distributed is `groupby`. \n",
    "\n",
    "Here is an example of using `groupby` to see how many houses fall into each category of `MSZoning`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('BedroomAbvGr').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From that we can see that there are 6 houses on a gravel road while all of the others are on a paved road. \n",
    "\n",
    "We could also look at the mean price of the outcome, `SalePrice`, for these different categories. If the means are quite different, than this might be a good indication that houses on gravel roads are simply different types of houses (and thus have lower or higher prices). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use group by on MSZoning to get the mean of SalePrice\n",
    "df.groupby('Street')['SalePrice'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q1: Use `groupby` on the feature, `BedroomAbvGr`, to see how many houses have 1 bedroom, 2 bedrooms, etc.***\n",
    "1. ***What seems to be the average, or most typical number of bedrooms a home has?*** \n",
    "2. ***How many houses in this dataset have this number of bedrooms?***\n",
    "\n",
    "\\<INPUT YOUR ANSWER TO Q1 HERE\\>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input your code to count how many houses are in each category of BedroomAbvGr then input your answer above where requested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't do it here, but plotting each feature individually, and plotting it against the response or target variable, is another helpful way to understand what is happening with the data. However, it can also be quite time consuming. \n",
    "\n",
    "There are now tools to help automate some of this. The first we'll mention is the VS Code extension, [Data Wrangler](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.datawrangler). This allows you to quickly look at the data and see what the distribution of each column looks like, see how may missing values each column has, how many distinct values, etc.\n",
    "\n",
    "If we wanted to use a tool that only depended on Python (and was not an IDE extension), then there are other options. \n",
    "The Python module, [YData-Profiling (formerly Pandas Profiling)](https://github.com/ydataai/ydata-profiling), even goes beyond what Data Wrangler does. Here is an [example of this EDA tool on the Titanic dataset](https://docs.profiling.ydata.ai/latest/examples/titanic/titanic_report.html). You should definitly consider using this type of tool for your course project in this class. \n",
    "\n",
    "Before we continue let's pick out a subset of columns that we'll use to model. In other words, wwe won't use all of the features right away, but we'll instead just use the numeric features, and perhaps one categorical feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the column names that are either integers or floats\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric_df = df.select_dtypes(include=numerics).copy()\n",
    "numeric_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column, Neighborhood from df and convert it to a one-hot encoded DataFrame\n",
    "neighborhood = df['Neighborhood']\n",
    "neighborhood = pd.get_dummies(neighborhood).astype(int)\n",
    "neighborhood.shape\n",
    "\n",
    "# Add the one-hot encoded DataFrame to numeric_df\n",
    "numeric_df = pd.concat([numeric_df, neighborhood], axis=1)\n",
    "numeric_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all of those 38 columns, which includes the outcome, SalesPrice, let's see which ones have missing values. We could manually look at this with Data Wrangler, but the following code cell quickly checks how many missing values each column has. \n",
    "\n",
    "Note that dealing with missing values is an entire subject on its own. We will touch on it a little more, but you would likely see more about it in an advanced Statistics or ML course. To give you a peek of what it entails, check out this reference on [Dealing with Missing Data](https://www.dasca.org/world-of-data-science/article/strategies-for-handling-missing-values-in-data-analysis). \n",
    "\n",
    "\n",
    "Since we are not going to deal with missing values directly (by trying to replace the missing values with a 'best guess'), we will simply exclude columns that have _too many_ missing values. Exactly how many is _too many_ is up to us, but we'll opt for a max of 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See which columns in numeric_df have more than 50 missing values\n",
    "missing = numeric_df.isnull().sum()\n",
    "missing = missing[missing > 50]\n",
    "print(missing)\n",
    "\n",
    "# Remove these columns from the DataFrame\n",
    "ncols_before = numeric_df.shape[1]\n",
    "numeric_df = numeric_df.drop(columns=missing.index)\n",
    "ncols_after = numeric_df.shape[1]\n",
    "print(f\"** Removed {ncols_before - ncols_after} columns **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now remove the rows from the data that have any missing values. Again, you may deal with missing values in a different way depending on how much data you have, which column it is that has missing values, etc. But, for us, we'll simply remove the rows with missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "nrows_before = numeric_df.shape[0]\n",
    "numeric_df = numeric_df.dropna()\n",
    "nrows_after = numeric_df.shape[0]\n",
    "print(f\"** Removed {nrows_before - nrows_after} rows **\")\n",
    "numeric_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now copy over the predictors (or features, or independent variables) to a __Pandas Data Frame__, `X`, and the outcome (or response, or dependent variable, target variable, etc.) to a __Pandas Data Series__, `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numeric_df.drop(columns=['Id','SalePrice']).copy() # remove the index Id and outcome SalePrice\n",
    "y = numeric_df['SalePrice'].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q2: What is the `shape` of X and y? In the following code cell input the code needed to see the shape of X and y. Then for your answer here state the number of rows and columns in each X and y. \n",
    "\n",
    "\\<INPUT YOUR ANSWER TO Q2 HERE\\>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the shape of X and y here\n",
    "#_____\n",
    "#_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll add a few more features to our data. Note that these are not entirely new features, but features that are transformations of the current features in some way. We're not going into much detail here, but there is also a lot that can be said about the practice of _[Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering)_, which is where you take the features you have and manipulate/combined/transform them in some way to create new features that (ideally) are able to predict y even better than the original features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to X for square root of 1stFlrSF, LotArea, GrLivArea, WoodDeckSF, OpenPorchSF, GarageArea\n",
    "X['sqrt_1stFlrSF'] = np.sqrt(X['1stFlrSF'])\n",
    "X['sqrt_LotArea'] = np.sqrt(X['LotArea'])\n",
    "X['sqrt_GrLivArea'] = np.sqrt(X['GrLivArea'])\n",
    "X['sqrt_WoodDeckSF'] = np.sqrt(X['WoodDeckSF'])\n",
    "X['sqrt_OpenPorchSF'] = np.sqrt(X['OpenPorchSF'])\n",
    "X['sqrt_GarageArea'] = np.sqrt(X['GarageArea'])\n",
    "X.shape\n",
    "\n",
    "# Add columns to X for the quadratic of 1stFlrSF, LotArea, GrLivArea, WoodDeckSF, OpenPorchSF, GarageArea\n",
    "X['quad_1stFlrSF'] = X['1stFlrSF']**2\n",
    "X['quad_LotArea'] = X['LotArea']**2\n",
    "X['quad_GrLivArea'] = X['GrLivArea']**2\n",
    "X['quad_WoodDeckSF'] = X['WoodDeckSF']**2\n",
    "X['quad_OpenPorchSF'] = X['OpenPorchSF']**2\n",
    "X['quad_GarageArea'] = X['GarageArea']**2\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now sort the columns of `X`, i.e. the features, so that the first column is the one with the highest correlation with `y`, the second column has the second highest correlation with `y`, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the columns of X according to how strong their correlation is with SalePrice\n",
    "correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "X = X[correlations.index]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q3: Does the feature engineering step seem like it was a good idea for this dataset of Home Sale Prices? Answer yes or no, and explain why.\n",
    "\n",
    "\\<INPUT YOUR ANSWER TO Q3 HERE - hint: use the output from the code cell above and the command, `X.head()`, to help explain your answer; namely, which columns have the highest correlation with y \\>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "\n",
    "Let's now try a few different types of models and try to \"_tune_~ each one. When we say \"_tune_\" a model, what we mean is that we want to find the ideal value of the model's [hyperparameter(s)](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)). Remember that a `hyperparameter` is a parameter of a model that is not fit/learned directly from the data. Rather, it is a value that determines _how the model will learn_ from the. Some examples are:\n",
    "* `max_depth` of a Decision Tree - the maximum depth of a decision tree is not directly learned from the data, but it will affect how the model learns, and ultimately how many parameters will be in the tree\n",
    "* `num_estimators` in an Ensemble model (e.g. Random Forest or GradientBoosting Tree) - again, this will affect how many total parameters will be in the final (ensemble) model\n",
    "\n",
    "Note that there are often times more than one hyperparameter. Below we are trying many values of a hyperparameter, e.g. $n_{h1}$ possible values. However, if there were multiple hyperparameters and the second one had $n_{h2}$ possible values, and the third had $n_{h3}$, ..., and the $k^{th}$ had $n_{hk}$ possible values, then there would be an a huge number of possible combinations that we would need to consider (namely, $n_{h1}\\cdot n_{h2}\\cdot \\dots \\cdot n_{hk}$). Fortunately, most models do not have too many hyperparameters to consider, or perhaps not many values of each to consider, so in practice this is typically not too onerous. But, be careful about trying to tune a model for every possible hyperparameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin to do any model training, remember that it is critical that we split the data into separate training and test datasets. \n",
    "\n",
    "Note that code below is splitting the data into 80% training data and 20% test data. This is the typical split although there may be times where a different proportion would make more sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Regression Model\n",
    "\n",
    "We'll now fit a basic Least Squares regression model to this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a regression model to the data using only the first column of X to predict y\n",
    "model_ls = LinearRegression()\n",
    "num_features = X.shape[1]\n",
    "\n",
    "rmse_values_ls = []\n",
    "pred_rmse_values_ls = []\n",
    "min_features = 1\n",
    "for i in range(min_features, num_features + 1):\n",
    "    model_ls.fit(X_train.iloc[:,0:i], y_train)\n",
    "    mse = np.sqrt(np.mean((y_train - model_ls.predict(X_train.iloc[:,0:i]))**2))\n",
    "    pred_mse = np.sqrt(np.mean((y_test - model_ls.predict(X_test.iloc[:,0:i]))**2))\n",
    "    rmse_values_ls.append(mse)\n",
    "    pred_rmse_values_ls.append(pred_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rmse values\n",
    "plt.plot(range(min_features, num_features + 1), rmse_values_ls, label='Train RMSE', color='blue')\n",
    "plt.plot(range(min_features, num_features + 1), pred_rmse_values_ls, label='Test RMSE', color='green')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('RMSE')\n",
    "plt.ylim(0, 60000)\n",
    "plt.title('Least Squares Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_values_dt = []\n",
    "pred_rmse_values_dt = []\n",
    "max_depths = range(1, 50)\n",
    "for i in range(len(max_depths)):\n",
    "    model_dt = DecisionTreeRegressor(max_depth=max_depths[i])\n",
    "    model_dt.fit(X_train, y_train)\n",
    "    mse = np.sqrt(np.mean((y_train - model_dt.predict(X_train))**2))\n",
    "    pred_mse = np.sqrt(np.mean((y_test - model_dt.predict(X_test))**2))\n",
    "    rmse_values_dt.append(mse)\n",
    "    pred_rmse_values_dt.append(pred_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rmse values\n",
    "plt.plot(range(min(max_depths), max(max_depths)+1), rmse_values_dt, label='Train RMSE', color='blue')\n",
    "plt.plot(range(min(max_depths), max(max_depths)+1), pred_rmse_values_dt, label='Test RMSE', color='green')\n",
    "plt.xlabel('Decision Tree Max Depth')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Decision Tree Regression')\n",
    "plt.ylim(0, 60000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_values_dt = []\n",
    "pred_rmse_values_dt = []\n",
    "poss_num_estimators = list(range(1, 102, 5))\n",
    "for i in range(len(poss_num_estimators)):\n",
    "    #print(f\"** num_estimators = {poss_num_estimators[i]} **\")\n",
    "    model_rf = RandomForestRegressor(n_estimators=poss_num_estimators[i], min_samples_leaf=1, random_state=2)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    mse = np.sqrt(np.mean((y_train - model_rf.predict(X_train))**2))\n",
    "    pred_mse = np.sqrt(np.mean((y_test - model_rf.predict(X_test))**2))\n",
    "    rmse_values_dt.append(mse)\n",
    "    pred_rmse_values_dt.append(pred_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rmse values\n",
    "plt.plot(poss_num_estimators, rmse_values_dt, label='Train RMSE', color='blue')\n",
    "plt.plot(poss_num_estimators, pred_rmse_values_dt, label='Test RMSE', color='green')\n",
    "plt.xlabel('Random Forest Number of Estimators (i.e. # of Decision Trees)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Random Forest Model Tuning')\n",
    "plt.ylim(0, 60000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a Gradient Boosting Regressor\n",
    "rmse_values_dt = []\n",
    "pred_rmse_values_dt = []\n",
    "poss_num_estimators = list(range(1, 402, 10))\n",
    "for i in range(len(poss_num_estimators)):\n",
    "    #print(f\"** num_estimators = {poss_num_estimators[i]} **\")\n",
    "    model_gb = GradientBoostingRegressor(n_estimators=poss_num_estimators[i], learning_rate=0.1, random_state=2)\n",
    "    model_gb.fit(X_train, y_train)\n",
    "    mse = np.sqrt(np.mean((y_train - model_gb.predict(X_train))**2))\n",
    "    pred_mse = np.sqrt(np.mean((y_test - model_gb.predict(X_test))**2))\n",
    "    rmse_values_dt.append(mse)\n",
    "    pred_rmse_values_dt.append(pred_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rmse values\n",
    "plt.plot(poss_num_estimators, rmse_values_dt, label='Train RMSE', color='blue')\n",
    "plt.plot(poss_num_estimators, pred_rmse_values_dt, label='Test RMSE', color='green')\n",
    "plt.xlabel('Gradient Boosting Regressor Number of Estimators (i.e. # of Trees)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Gradient Boosting Regressor Model Tuning')\n",
    "plt.ylim(0, 60000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q4: Based on the results above, which model would you choose, and which hyperparameter value for that model would you use? Explain your answer. \n",
    "\n",
    "\\<INPUT YOUR ANSWER TO Q4 HERE\\>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating a Model's Future Accuracy\n",
    "\n",
    "From the above we might have a decent idea of how well a model will perform on future data. After all, we did have separate `train` and `test` datasets, so it seems like we can confidently say what the accuracy will be on future data. \n",
    "\n",
    "However, we have to remember that our `test` dataset is just one possible example of future data that our model might encounter. We know that the accuracy on a different _future_ dataset will not be the exact same as the accuracy we saw above on the test dataset. Therefore, what we really need to do is to _estimate_ what our model's future accuracy will be. Which means that we have to get an idea of how much the accuracy (or RMSE) will vary from one _test_ dataset to another. As you know from basic Statistics, this sounds a lot like trying to undestand the variance of the model's performance metric for all possible future test datasets. \n",
    "\n",
    "But, where can we get more test data? We need training data, and ideally the more training data the better. So, we can't simply just change the training/test split from, say, 80/20, to be 50/50 or 20/80. \n",
    "What can we do? This is where the idea of **[Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))** comes into play. Let's now use Scikit Learn's `cross_val_score` function to split our data one time into one possible training and test split; then do it again with a different training and test split; then again, up to $k$ possible times. This is called **K-fold Cross-validation** and it gives us **k** different test datasets of size $\\frac{1}{k}\\cdot n$, where $n$ is the size of our original dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set k to be the number of folds for cross-validation (i.e. the number of different test dataset we'll use)\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ls = LinearRegression()\n",
    "scores_ls = cross_val_score(model_ls, X.iloc[:,:30], y, cv=k, scoring='r2')\n",
    "print(f\"** Average R2 = {scores_ls.mean():1.2E} **\")\n",
    "print(f\"** R2 SD = {np.sqrt(scores_ls.var()):1.2E} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = DecisionTreeRegressor(max_depth=20, random_state=2)\n",
    "scores_dt = cross_val_score(model_dt, X, y, cv=k, scoring='r2')\n",
    "print(f\"** Average R2 = {scores_dt.mean():1.2E} **\")\n",
    "print(f\"** R2 SD = {np.sqrt(scores_dt.var()):1.2E} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=2)\n",
    "scores_rf = cross_val_score(model_rf, X, y, cv=k, scoring='r2')\n",
    "print(f\"** Average R2 = {scores_rf.mean():1.2E} **\")\n",
    "print(f\"** R2 SD = {np.sqrt(scores_rf.var()):1.2E} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=2)\n",
    "scores_gb = cross_val_score(model_gb, X, y, cv=k, scoring='r2')\n",
    "print(f\"** Average R2 = {scores_gb.mean():1.2E} **\")\n",
    "print(f\"** R2 SD = {np.sqrt(scores_gb.var()):1.2E} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(scores_ls.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot score means and variances on a bar chart with a line for the variance\n",
    "models = ['Linear Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting']\n",
    "means = [scores_ls.mean(), scores_dt.mean(), scores_rf.mean(), scores_gb.mean()]\n",
    "stderrors = [scores_ls.std(), scores_dt.std(), scores_rf.std(), scores_gb.std()] / np.sqrt(k)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "#ax.bar(models, means, yerr=variances, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.bar(models, means, align='center', alpha=0.5, ecolor='black', capsize=10, width=0.5)\n",
    "ax.errorbar(models, means, yerr=1.96*stderrors, fmt='o', color='black', ecolor='black', capsize=10)\n",
    "ax.set_ylabel(f'Mean R-squared and Approximate 95% CI')\n",
    "ax.set_title(f'Model Performance (using {k}-fold CV)')\n",
    "ax.yaxis.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have a relatively small sample size to estimate $R^2$ (in this case, k, the number of folds) we can still use the typical approach to point estimation and standard errors to get an idea of what the 95% confidence interval is. The above plot shows the mean $R^2$ and approximate confidence intervals. Confirming what we suspected before, it does seem that the Gradient Boosting Tree(s) is the best choice. \n",
    "\n",
    "### Thorough Model Tuning\n",
    "As you can imagine, just eyeballing the best hyperparameter value for a model based on curves like the ones we plotted above is not the most scientific. Oftentimes, once we have selected a model, we will want to try several different hyperparameter values, and perhaps combinations of the various hyperparameters associated with a model. \n",
    "\n",
    "For Gradient Boosting there are two important hyperparameters: \n",
    "* num_estimators - number of trees (linked together sequentially)\n",
    "* learning_rate - the amount of importance/weight each successive tree will decrease by\n",
    "\n",
    "We'll now use `GridSearch Cross-validation` to find the best values of these hyperparameters in a more robust way than we did earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, use grid search to find the best hyperparameters for the Gradient Boosting Regressor\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(1, 402, 50)),\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "model_gb = GradientBoostingRegressor(random_state=2)\n",
    "grid_search = GridSearchCV(model_gb, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f\"** Best R2 = {grid_search.best_score_:1.2E} **\")\n",
    "print(f\"** Best Parameters = {grid_search.best_params_} **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a learning rate of 0.05 and ~400 trees will give us the best possible model. Let's now plot all the $R^2$ values for each combination of hyperparameter values that were tried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a heatmap of the R2 values for the different hyperparameters\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "scores = np.array(results.mean_test_score).reshape(len(param_grid['n_estimators']), len(param_grid['learning_rate']))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores, cmap='viridis')\n",
    "#plt.imshow(scores, interpolation='nearest', cmap='viridis')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('n_estimators')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(param_grid['learning_rate'])), param_grid['learning_rate'], rotation=45)\n",
    "plt.yticks(np.arange(len(param_grid['n_estimators'])), param_grid['n_estimators'])\n",
    "plt.title('R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q5: You should now have a good level of understanding of selecting and tuning a model. For this last question, go ahead and try out a new model type of your choice. Do **not** use one of the models above, but instead select one from the list below. Once you've done that, go ahead and run `grid search cross-validation` on the model and see if you can find one that fits the data better than the Gradient Boosting Tree above. The qustion is then, what is the peformance of your model? (i.e. what is the $R^2$?) \n",
    "\n",
    "\\<INPUT YOUR ANSWER TO Q5 HERE - i.e. what model did you choose and what was the $R^2$?\\>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of models that we have not explicitly talked about in class (although most are specialized versions of linear models or boosting ensemble models):\n",
    "\n",
    "* [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor)\n",
    "* [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)\n",
    "* [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor)\n",
    "* [KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) - we looked at K-nearest neighbors, so this might be a good one to try, remember what the hyperparameter $k$ does when it comes to increasing/decreasing model complexity\n",
    "* [Linear Models with automatica Variable Selection](https://scikit-learn.org/stable/api/sklearn.linear_model.html#regressors-with-variable-selection) - we won't cover these much, but this is something you would discuss in a Statistical Methods course, or other course on traditional statistical modeling methods\n",
    "* [Support Vector Regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) - we haven't yet talked about Support Vector Machines but you can still use it here if you want to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the new model you have chosen\n",
    "\n",
    "# declare an instance of your model\n",
    "model_xx = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a grid or range of hyperparameter values to search over\n",
    "param_grid = {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(model_xx, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f\"** Best R2 = {grid_search.best_score_:1.2E} **\")\n",
    "print(f\"** Best Parameters = {grid_search.best_params_} **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you want to do a little bit extra to make your model even better, you could try one (or both) of the following:\n",
    "1. Add some of the categorical features that were left out earlier, which you'll likely need to encode the same way we did above with `Neighborhood`\n",
    "2. Do some more feature engineering to create new features from the existing ones (e.g. interactions, new categorical variables based on old ones, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
